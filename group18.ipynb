{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CW2: Neural Computation - Convolutional Neural Networks<a href=\"#CW2:-Neural-Computation---Convolutional-Neural-Networks\" class=\"anchor-link\">¶</a>\n",
    "===================================================================================================================================================\n",
    "\n",
    "### Deadline: December 18th at 5pm, 2020.<a href=\"#Deadline:-December-18th-at-5pm,-2020.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "### Adnaan Jhetam 2244790, Shehryar Ahmad, Jacob Norman 1680946, Sean Duignan2257153, Chengyi Ju 2095773, Venkata Lahari Balantrapu 2218037<a href=\"#Adnaan-Jhetam-2244790,-Shehryar-Ahmad,-Jacob-Norman-1680946,-Sean-Duignan2257153,-Chengyi-Ju-2095773,-Venkata-Lahari-Balantrapu-2218037\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "1 Introduction<a href=\"#1-Introduction\" class=\"anchor-link\">¶</a>\n",
    "=================================================================\n",
    "\n",
    "In this report, the design and implementation of a CNN was assessed in\n",
    "terms of its performance in semantic segmentation of MR Images.\n",
    "Convolutional Neural Networks have been shown to aid in image\n",
    "recognition machine learning problems often used due to their simple\n",
    "framework. (O’Shea and Nash, 2015). The basic architecture of the CNN\n",
    "always has convolutional, nonlinearity, pooling and fully connected\n",
    "layers. Convolutional Neural Networks advantage over Artificial Neural\n",
    "Networks is that it reduces the number of hyperparameters, which meant\n",
    "that more complicated tasks could be solved using bigger models.(Albawi\n",
    "and Mohamed, 2017).\n",
    "\n",
    "1.1 Aim:<a href=\"#1.1-Aim:\" class=\"anchor-link\">¶</a>\n",
    "-----------------------------------------------------\n",
    "\n",
    "The aim of this investigation was to develop a CNN that can section an\n",
    "MR image of a heart into 4 sectors:\n",
    "\n",
    "-   the background\n",
    "-   the left ventricle\n",
    "-   the right ventricle\n",
    "-   the myocardium  \n",
    "\n",
    "Once the neural network had been trained, certain hyperparameters were\n",
    "then modified in order to increase the accuracy of the results.\n",
    "\n",
    "1.2 Objectives:<a href=\"#1.2-Objectives:\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "1.  Design and train a CNN neural network\n",
    "2.  Optimize the neural network for the specific task.\n",
    "3.  Analyse & display the data obtained to acquire the optimal network\n",
    "    hyperparameters.\n",
    "4.  Group code submission on Kaggle\n",
    "\n",
    "2 Background<a href=\"#2-Background\" class=\"anchor-link\">¶</a>\n",
    "=============================================================\n",
    "\n",
    "2.1 Batch Size in CNN<a href=\"#2.1-Batch-Size-in-CNN\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "A key parameter to alter in convolutional neural networks is the batch\n",
    "size. Batch size has a significant impact in the generalized performance\n",
    "of the network, and can affect both the time it takes to run the\n",
    "iteration and the accuracy. The batch size is used for the estimation of\n",
    "the gradient and is the number of images used, and the main test is to\n",
    "assess whether using a larger or smaller batch size will have any\n",
    "consequence.(Luschi, 2016). Both smaller and larger batch sizes have\n",
    "their advantages and disadvantages. Using a larger number of images\n",
    "would mean that it reaches a better minima than a smaller size. However\n",
    "using a smaller size affects the regularization due to higher variance\n",
    "in conjunction with adjusting the learning rate to stop any\n",
    "overshooting. Furthermore a smaller batch converges quicker when\n",
    "compared to a larger batch size.(Radiuk, 2017).\n",
    "\n",
    "2.2 Loss Function in CNN<a href=\"#2.2-Loss-Function-in-CNN\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "As part of the optimization algorithm, the error for the current state\n",
    "of the model must be estimated repeatedly. This requires the choice of\n",
    "an error function, conventionally called a loss function, that can be\n",
    "used to estimate the loss of the model so that the weights can be\n",
    "updated to reduce the loss on the next evaluation.(Jason Brownlee, 2019)\n",
    "Cross-entropy Loss is also called log-likelihood Loss. The cross entropy\n",
    "loss function is expressed as:\n",
    "\n",
    "\\$L = -\\\\sigma (y\\_i \\* log(x\\_i))\\$\n",
    "\n",
    "The function in Pytorch is not a cross entropy loss function in a strict\n",
    "sense. It first activates the input through softmax activation function,\n",
    "normalizes the vector into a probability form, and then calculates the\n",
    "cross entropy loss with target.\n",
    "\n",
    "2.3 Optimizers in CNN<a href=\"#2.3-Optimizers-in-CNN\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "Gradient descent optimization algorithms, while increasingly popular,\n",
    "are often used as black-box optimizers, as practical explanations of\n",
    "their strengths and weaknesses are hard to come by. Gradient descent is\n",
    "a way to minimize an objective function \\$J(θ)\\$ parameterized by a\n",
    "model’s parameters \\$θ ∈ R^d \\$by updating the parameters in the\n",
    "opposite direction of the gradient of the objective function \\$∇θJ(θ)\\$\n",
    "w.r.t. to the parameters. (Sebastian Ruder, 2017) The state-of-the-art\n",
    "gradient-descent based algorithms such as Adagrad, Adam, SGD, RMSprop,\n",
    "AdaMax, Adadelta, etc., have different effects on the performance of any\n",
    "model. Therefore, the selection process of an ideal optimizer is\n",
    "necessary and this is done by testing the model with four of these\n",
    "optimizers.\n",
    "\n",
    "2.4 Learning Rate in CNN<a href=\"#2.4-Learning-Rate-in-CNN\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "The learning rate plays a major role in the neural networks as it\n",
    "controls the amount of change in the weight over every update. A\n",
    "learning rate that is too large can be as slow as a learning rate that\n",
    "is too small, and a learning rate that is too large or too small can\n",
    "require orders of magnitude more training time than one that is in an\n",
    "appropriate range (Tony R. Martinez, 2001). An optimised learning rate\n",
    "henceforth would lead to faster convergence with minimum cost or error\n",
    "rates.\n",
    "\n",
    "2.5 Architecture in CNN<a href=\"#2.5-Architecture-in-CNN\" class=\"anchor-link\">¶</a>\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "The architecture of a convolutional neural network is the set of\n",
    "convolutions and related functions that will be undertaken on the images\n",
    "being passed through the network. Naturally, it has the ability to\n",
    "heavily affect the performance of the network as it defines the network\n",
    "itself. For a high performance, the developer must not make the\n",
    "architecture too simple, as this will leave it incapable to capture the\n",
    "features of the image, preventing any accurate predictions. The\n",
    "architecture must also not be too complex, as this will lead to heavy\n",
    "computational costs and invite the risk of overfitting.\n",
    "\n",
    "3 Implementation<a href=\"#3-Implementation\" class=\"anchor-link\">¶</a>\n",
    "=====================================================================\n",
    "\n",
    "The neural network used for this task is based on a “U-Net”, proposed by\n",
    "Ronneberger et al. 2015 (Ronneberger et al. 1). U-Net is a fully\n",
    "convolutional network, designed specifically for biomedical image\n",
    "segmentation. As such, there was very little modification required to\n",
    "use U-Net for this task. The network can be broken down into two\n",
    "distinct parts, referred to by Ronneberger et al. as a contracting path\n",
    "and a symmetric expanding path. Figures 1 and 2 show the U-Net\n",
    "architecture and the modified architecture used for this task.\n",
    "\n",
    "The contracting path is a combination of convolutional, normalisation,\n",
    "ReLu, and max pooling layers. This part of the networks is structured as\n",
    "\n",
    "1.  An initial step consisting of two convolutions, each followed by a\n",
    "    normalisation layer and a ReLu layer. This is referred to as a\n",
    "    double convolution.\n",
    "2.  4 downsampling steps, each consisting of a max pooling layer,\n",
    "    followed by a double convolution.\n",
    "\n",
    "The expanding path consists of upsampling layers, copy operations and\n",
    "convolutional layers. Upsampling layers are convolutions that half the\n",
    "size of the tensor. The copy operation combines the tensor with the\n",
    "corresponding vector shown in figures 1 and 2, doubling the first\n",
    "dimension of the tensor. The structure is\n",
    "\n",
    "1.  An upsampling layer, followed by a concatenation operation and a\n",
    "    double convolution. This step is repeated 4 times.\n",
    "2.  A 1x1 convolution that produces a 4x96x96 output tensor.\n",
    "\n",
    "<img src=\"https://i.imgur.com/eXmcPcV.png\" />\n",
    "\n",
    "There are two notable differences between the architecture used for this\n",
    "project and the U-Net architecture. Firstly, a normalisation layer was\n",
    "added in the double convolution operations to improve network training\n",
    "time. In addition, a padding of size 1 was used in the double\n",
    "convolution operation in the implementation n for this project. This\n",
    "simplified the structure of the network because the shape of the tensor\n",
    "was unchanged except for the first dimension. A benefit of this was seen\n",
    "in the copy operation; since the tensors were all standard size, the\n",
    "copied tensor did not need to be cropped before the tensors were\n",
    "concatenated.\n",
    "\n",
    "To implement the network, 4 classes were defined that grouped multiple\n",
    "operations together for simplicity:\n",
    "\n",
    "1.  A DoubleConv class, with two repeats of a convolution layer, a\n",
    "    normalisation layer and a ReLu layer.\n",
    "2.  A Down class, with a max pooling layer and a DoubleConv operation.\n",
    "3.  An Up class, with an upsampling layer, a copy operation, and a\n",
    "    convolution layer.\n",
    "4.  An OutConv class, with a single convolution layer.\n",
    "\n",
    "<img src=\"https://i.imgur.com/uKNQh0f.png.png\" />\n",
    "\n",
    "<img src=\"https://i.imgur.com/I6VJiyh.png.png\" />\n",
    "<img src=\"https://i.imgur.com/JNH3DS8.png\" />\n",
    "\n",
    "The full network was implemented in a CNNSEG class, shown in Figure 4.\n",
    "The model was trained using the code shown in Figure 5. After each\n",
    "epoch, the loss was recorded in a csv file. Each trained model was\n",
    "evaluated on the evaluation set using the dice score (shown in Figure\n",
    "6).\n",
    "\n",
    "<img src=\"https://i.imgur.com/gwPW3Cz.png\" />\n",
    "\n",
    "4 Experiment<a href=\"#4-Experiment\" class=\"anchor-link\">¶</a>\n",
    "=============================================================\n",
    "\n",
    "The experimental process to measure the generalization performance of\n",
    "the model consisted of 5 main metrics. The investigation was split to\n",
    "assess how the loss function, the batch size, the architecture of the\n",
    "network and the learning rate and optimizer affects the dice score\n",
    "values. The epoch value was kept at 100.\n",
    "\n",
    "To carry out the experiment to optimize the neural networks\n",
    "generalization performance, the following series of tests were\n",
    "initiated:\n",
    "\n",
    "-   To observe the effects on how batch size affects the overall mean\n",
    "    dice score and mean loss, the batch size was altered with values\n",
    "    \\[1,5,10,20,50,75,100\\].\n",
    "\n",
    "-   To investigate how the Loss function affects the mean dice score,\n",
    "    the loss function was changed from CrossEntropyLoss to NLLLoss.\n",
    "\n",
    "-   To investigate the performance of the model and the classification\n",
    "    accuracy, the epochs were adjusted to three values of 40, 50 and 100\n",
    "    compared to the original value of 5.\n",
    "\n",
    "-   To investigate how network architecture affects the mean dice score,\n",
    "    four different network architectures were experimented with. These\n",
    "    were all slight variants of the modified U-Net architecture\n",
    "    described in the previous section. The first variant, Architecture\n",
    "    1, was altered by the removal of the 6x6 layer, making the 12x12\n",
    "    layer the smallest in the network. Architecture 2 takes this change\n",
    "    one step further; the 12x12 layer was removed to make the 24x24\n",
    "    layer the new smallest. Architecture 3 is a variant of the modified\n",
    "    U-Net in which the amount of channels in each of the hidden layers\n",
    "    is halved; the 96x96 layer with 64 channels is changed to a 96x96\n",
    "    layer with 32 channels, and so on. Architecture 4 is the same,\n",
    "    except the amount of channels is doubled instead of halved; the\n",
    "    96x96 layer with 64 channels is changed to a 96x96 layer with 128\n",
    "    channels, and so on.\n",
    "\n",
    "-   To investigate how the model was affected by the optimisers, we\n",
    "    implemented four out of the several optimisers i.e., Adam, SGD,\n",
    "    ASGD, RMSprop already present in the torch library. The learning\n",
    "    rate was tested with a range of values between 0.001 and 0.5, to\n",
    "    investigate how it affected the classification accuracy.\n",
    "\n",
    "5 Results<a href=\"#5-Results\" class=\"anchor-link\">¶</a>\n",
    "=======================================================\n",
    "\n",
    "5.1 Loss Function Changes<a href=\"#5.1-Loss-Function-Changes\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "<img src=\"https://i.imgur.com/Biia4F8.png\" />\n",
    "\n",
    "The Loss function was changed to investigate how it affects the mean\n",
    "dice score. As shown in Table 1, the Cross EntropyLoss performs better\n",
    "than the NLLLoss function. It is shown that the model using\n",
    "CrossEntropyLoss as the Loss function can obtain a smaller mean loss\n",
    "value. The mean dice score for the first class(the left ventricle region\n",
    "of the images) obtained by using the CrossEntropyLoss function is 0.8058\n",
    "while the score obtained by NLLLoss function is much lower, which is\n",
    "only 0.5671. Therefore the mean dice score across all classes using\n",
    "CrossEntropyLoss function is a bit higher, which is about 0.8639.\n",
    "\n",
    "5.2 Batch Size Changes<a href=\"#5.2-Batch-Size-Changes\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "<img src=\"https://i.imgur.com/rHUbotb.png\" />  \n",
    "\n",
    "<img src=\"https://i.imgur.com/EGnxCi4.png\" />\n",
    "\n",
    "The batch size parameter was adjusted to observe the effects it has on\n",
    "the dice score over 100 iterations. As shown in the figure above\n",
    "showcasing the increasing batch size. The line graph clearly signified\n",
    "that as the batch increases, the score variable exponentially increases\n",
    "until it reaches a plateau at around 0.86. During the process, it can be\n",
    "seen that using a lower batch size between 20 to 50 could be used as it\n",
    "reached high values of around 0.85 . For the graph of Mean loss Vs Batch\n",
    "Size it shows a positive correlation trend between the two, increasing\n",
    "from 0.5 to 1.03. The slight dip at batch size 15 could be regarded as\n",
    "anomalous as it does not fit the trend. The graph does seem to level and\n",
    "decrease at 75 and 100 batch sizes.\n",
    "\n",
    "5.3 Architecture changes<a href=\"#5.3-Architecture-changes\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "The 5 architectures were tested with the other hyper parameters fixed\n",
    "as: learning rate = 0.1, batch size = 12, loss function = cross entropy,\n",
    "and optimiser = Adam. Due to time consumption factors, each architecture\n",
    "was initially only trained for 10 epochs, then the best of the variants\n",
    "was selected for a full comparison with modified U-Net over 100 epochs.\n",
    "The table below shows the Mean Loss value and Mean Dice Score after the\n",
    "10th epoch for each of the architectures. Architecture 4 is not included\n",
    "as it was too complex to train in a reasonable time frame; several hours\n",
    "were required to complete a single epoch.\n",
    "\n",
    "<img src=\"https://i.imgur.com/hpyRknv.png\" />\n",
    "<img src=\"https://i.imgur.com/IpdLII4.png\" />\n",
    "\n",
    "Based on these initial results, Architecture 2 was chosen for the full\n",
    "training of 100 epochs due to its relatively high dice score.The\n",
    "resulting loss values and mean dice scores for modified U-Net and\n",
    "Architecture 2 after 100 epochs are given below.\n",
    "\n",
    "<img src=\"https://i.imgur.com/lMcMYRf.png\" />\n",
    "\n",
    "As the results show, the modified U-Net architecture is able to achieve\n",
    "a higher dice score than Architecture 2, despite also having a higher\n",
    "loss.\n",
    "\n",
    "5.4 Optimizer changes and learning rate<a href=\"#5.4-Optimizer-changes-and-learning-rate\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The model’s behaviour under Adam, SGD, ASGD, RMSprop has been studied\n",
    "where the batch size and epochs have been fixed to 12 and 40. The\n",
    "learning rates were altered between 0.1 and 0.01. The changes in the\n",
    "mean dice score have been observed. These, along with their mean dice\n",
    "metric score are outlined in Table 1.\n",
    "\n",
    "<img src=\"https://i.imgur.com/BrorbDZ.png\" />\n",
    "\n",
    "The learning rates higher than 0.1 gave unstable results as the batch\n",
    "size is comparatively small and they reached a sub-optimal too fast.\n",
    "However, learning rates as low as 0.001 required a very high training\n",
    "time to converge. Among the four optimizers tested, the RMSprop seemed\n",
    "to have given the least mean dice score of 0.683 whereas SGD and Adam\n",
    "optimizers had a similar performance under the learning rate of 0.1 with\n",
    "a mean dice score of around 0.854. Furthermore, The ASGD resulted in a\n",
    "mean dice score of 0.8708 and has been chosen for the final model.\n",
    "\n",
    "5.5 Optimal CNN<a href=\"#5.5-Optimal-CNN\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "To achieve an optimal CNN a further set of experiments has been\n",
    "conducted with the proposed batch sizes ASGD as the optimiser along with\n",
    "the learning rates. The observed mean dice score for these set of\n",
    "experiments have been recorded in the table 6.\n",
    "\n",
    "<img src=\"https://i.imgur.com/EQYrxTG.png\" />\n",
    "\n",
    "It has been observed that the highest mean dice score was achieved with\n",
    "a batch size of 12. The learning rates of 0.08 and 0.1 as well as a\n",
    "difference of 10 epochs had no much difference on the performance on the\n",
    "model as both of them resulted in a mean dice score of 0.87. However, a\n",
    "learning rate of 0.08 with 50 epochs is the best choice as it gave the\n",
    "overall best performance.\n",
    "\n",
    "6 Discussion<a href=\"#6-Discussion\" class=\"anchor-link\">¶</a>\n",
    "=============================================================\n",
    "\n",
    "6.1 Loss function changes<a href=\"#6.1-Loss-function-changes\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "The Pytorch provides a variety of readymade Loss functions. For\n",
    "multi-class classification problems, the three most commonly used\n",
    "functions are CrossEntropyLoss, NLLLoss and KLDivLoss. However, the four\n",
    "classes in the network have different sizes so each class has a\n",
    "different weight. KLDivLoss is not suitable for this because it can not\n",
    "run with a keyword argument “weight”. As for the NLLLoss\n",
    "function(Negative Log Likelihood), the main difference between it and\n",
    "the CrossEntropyLoss function is that the nn.CrossEntropyLoss()\n",
    "criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "From the results shown above it can be seen that using CrossEntropyloss\n",
    "function directly is better than adding a LogSoftmax layer in the last\n",
    "layer of the network and using the NLLLoss function.\n",
    "\n",
    "6.2 Batch size changes<a href=\"#6.2-Batch-size-changes\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "From the results shown, the batch size has been shown to drastically\n",
    "affect the accuracy performance when compared to other hyperparameters.\n",
    "A default parameter between 20 and 50 can be used for optimal\n",
    "performance, but increasing it any further led to a fairly constant mean\n",
    "dice value. The only factor that was affected by increasing the batch\n",
    "size to above 50 was the time it took to converge. The graph of Loss vs\n",
    "batch size has shown that the error increased as the batch size\n",
    "increased, therefore a compromise had to be made to obtain both a low\n",
    "error and a high enough score. Therefore, this parameter will decrease\n",
    "the accuracy if it is only used at below 5 To obtain the optimum mean\n",
    "dice score, a value of between 10 to 20 was used as this had a\n",
    "compromise loss of 0.79 at the 100th epoch and a mean dice score of\n",
    "0.854.\n",
    "\n",
    "6.3 Architecture changes<a href=\"#6.3-Architecture-changes\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "The initial 10 epoch testing showed a fairly consistent decline for each\n",
    "of the 4 architectures. Modified U-Net achieved the lowest loss value,\n",
    "0.34096, but had only the third highest mean dice score, 0.45626.\n",
    "Despite having the highest loss, 0.53724, Architecture 2 managed to\n",
    "achieve the highest mean dice score, 0.49425. The further test results\n",
    "of 100 epochs on the modified U-Net and Architecture 2 showed that,\n",
    "despite still displaying a higher loss value, the original modified\n",
    "U-Net is the most optimal tested architecture.\n",
    "\n",
    "6.4 Optimizer and learning rate changes<a href=\"#6.4-Optimizer-and-learning-rate-changes\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "From the results it can be seen that for each optimizer a learning rate\n",
    "of 0.1 outperformed a learning rate of 0.01. Thus, the ideal learning\n",
    "rate lies in the 0.1 range. This means that the rate of 0.1 converges\n",
    "most efficiently to the minimum of the loss function out of all the\n",
    "explored rates. Furthermore, the best performing optimizer is the ASGD\n",
    "optimizer with a mean dice score of 0.871 . This is followed by Adam,\n",
    "SGD and then RMSProp. The averaged stochastic gradient descent (ASGD)\n",
    "algorithm is the standard SGD optimizer which records an average of its\n",
    "parameter vector over time. When optimization has been performed, ASGD\n",
    "takes this averaged parameter vector as its final parameters. This\n",
    "results in an overall higher dice score.\n",
    "\n",
    "7 Conclusion<a href=\"#7-Conclusion\" class=\"anchor-link\">¶</a>\n",
    "=============================================================\n",
    "\n",
    "This project aimed to create a network to segment MR images of hearts\n",
    "into 4 sections. A U-Net based architecture was chosen for the task.\n",
    "Experiments were performed to optimise the hyperparameters of the\n",
    "network and compare different architectures. The modified U-Net\n",
    "architecture was found to perform better than other architectures\n",
    "tested. The hyper parameters that produced the highest performing\n",
    "network were a batch size of 12, a learning rate of 0.1, 50 training\n",
    "epochs, a cross entropy loss function, and an ASGD optimiser. Using this\n",
    "combination of hyper parameters, the network was able to achieve a final\n",
    "Mean Dice Score of 0.87270 on the validation data set.\n",
    "\n",
    "8 Code<a href=\"#8-Code\" class=\"anchor-link\">¶</a>\n",
    "=================================================\n",
    "\n",
    "Instructions for running code<a href=\"#Instructions-for-running-code\" class=\"anchor-link\">¶</a>\n",
    "-----------------------------------------------------------------------------------------------\n",
    "\n",
    "To run the code, 3 subfolders need to be added to the CW2 folder\n",
    "uploaded in the coursework description:\n",
    "\n",
    "-   a \"model\\_saves\" folder, containing a \"loss\\_values\" folder.\n",
    "-   a \"predicted\\_masks\" folder\n",
    "-   a \"submission\" folder\n",
    "\n",
    "Also, pandas and numpy need to be downloaded.\n",
    "\n",
    "1 Loading and visualising data<a href=\"#1-Loading-and-visualising-data\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------------------\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask, cmap=cmap)\n",
    "        plt.axis('off')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import os\n",
    "    import cv2 #import OpenCV\n",
    "\n",
    "    data_dir = './data/train'\n",
    "    image = cv2.imread(os.path.join(data_dir,'image','cmr1.png'), cv2.IMREAD_UNCHANGED)\n",
    "    mask = cv2.imread(os.path.join(data_dir,'mask','cmr1_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "    show_image_mask(image, mask, cmap='gray')\n",
    "    plt.pause(1)\n",
    "    cv2.imwrite(os.path.join('./','cmr1.png'), mask*85)\n",
    "\n",
    "### 1.1 DataLoader<a href=\"#1.1-DataLoader\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import torch\n",
    "    import torch.utils.data as data\n",
    "    import cv2\n",
    "    import os\n",
    "    from glob import glob\n",
    "\n",
    "    class TrainDataset(data.Dataset):\n",
    "        def __init__(self, root=''):\n",
    "            super(TrainDataset, self).__init__()\n",
    "            self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "            self.mask_files = []\n",
    "            for img_path in self.img_files:\n",
    "                basename = os.path.basename(img_path)\n",
    "                self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "                \n",
    "\n",
    "        def __getitem__(self, index):\n",
    "                img_path = self.img_files[index]\n",
    "                mask_path = self.mask_files[index]\n",
    "                data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "                label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "                return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.img_files)\n",
    "\n",
    "    class TestDataset(data.Dataset):\n",
    "        def __init__(self, root=''):\n",
    "            super(TestDataset, self).__init__()\n",
    "            self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "                img_path = self.img_files[index]\n",
    "                data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "                return torch.from_numpy(data).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.img_files)\n",
    "\n",
    "2 Training<a href=\"#2-Training\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------\n",
    "\n",
    "### 2.1 Segmenatation Model<a href=\"#2.1-Segmenatation-Model\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class DoubleConv(nn.Module):\n",
    "        \"\"\"(Convolution layer, normalisation layer, ReLu layer) * 2 \"\"\"\n",
    "\n",
    "        def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "            super().__init__()\n",
    "            if not mid_channels:\n",
    "                mid_channels = out_channels\n",
    "            self.double_conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(mid_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.double_conv(x)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    class Down(nn.Module):\n",
    "        \"\"\"Max pooling layer then DoubleConv\"\"\"\n",
    "\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super().__init__()\n",
    "            self.maxpool_conv = nn.Sequential(\n",
    "                nn.MaxPool2d(2),\n",
    "                DoubleConv(in_channels, out_channels)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.maxpool_conv(x)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    class Up(nn.Module):\n",
    "        \"\"\"Upsample then double conv\"\"\"\n",
    "\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super().__init__()\n",
    "\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "\n",
    "        def forward(self, x1, x2):\n",
    "            x1 = self.up(x1)\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "            return self.conv(x)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    class OutConv(nn.Module):\n",
    "        \"\"\" Single convolution layer\"\"\"\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(OutConv, self).__init__()\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.conv(x)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    class CNNSEG(nn.Module):\n",
    "        \"\"\" Modified U-Net CNN\"\"\"\n",
    "        def __init__(self):\n",
    "            super(CNNSEG, self).__init__()\n",
    "            self.initial = DoubleConv(1, 64)\n",
    "            self.down1 = Down(64, 128)\n",
    "            self.down2 = Down(128, 256)\n",
    "            self.down3 = Down(256, 512)\n",
    "            self.down4 = Down(512, 512)\n",
    "            self.up1 = Up(1024, 256)\n",
    "            self.up2 = Up(512, 128)\n",
    "            self.up3 = Up(256, 64)\n",
    "            self.up4 = Up(128, 64)\n",
    "            self.outc = OutConv(64, 4)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x1 = self.initial(x)\n",
    "            x2 = self.down1(x1)\n",
    "            x3 = self.down2(x2)\n",
    "            x4 = self.down3(x3)\n",
    "            x5 = self.down4(x4)\n",
    "            x = self.up1(x5, x4)\n",
    "            x = self.up2(x, x3)\n",
    "            x = self.up3(x, x2)\n",
    "            x = self.up4(x, x1)\n",
    "            x = self.outc(x)\n",
    "            return x\n",
    "\n",
    "### 2.2 Loss tracker functions<a href=\"#2.2-Loss-tracker-functions\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Helper functions to save and read loss to csv file when training\n",
    "\n",
    "    def save_loss(loss_log, model_name, print_last=True):\n",
    "        epochs = np.arange(1, len(loss_log)+1).reshape((-1, 1))\n",
    "        loss = np.asarray(loss_log)\n",
    "        mean_loss = np.mean(loss, axis=1).reshape((-1, 1))\n",
    "\n",
    "        loss_data = np.concatenate((epochs, loss, mean_loss), axis=1)\n",
    "        loss_df = pd.DataFrame(loss_data, columns = [\"Epoch\"]+[\"Batches \"+str(i)+\" loss\" for i in range(len(loss_log[0]))]+[\"Mean loss\"])\n",
    "        loss_df = loss_df.set_index(\"Epoch\")\n",
    "        loss_df.to_csv(\"model_saves/loss_values/\"+model_name+\"_loss.csv\")\n",
    "\n",
    "        if print_last:\n",
    "            print(\"Mean loss for epoch \", epochs[-1, 0], \": \", np.round(mean_loss[-1, 0], 4))\n",
    "\n",
    "    def retrieve_loss(model_name):\n",
    "        loss_df = pd.read_csv(\"model_saves/loss_values/\"+model_name+\"_loss.csv\")\n",
    "        loss_log = loss_df.drop(columns=['Epoch', 'Mean loss']).to_numpy().tolist()\n",
    "        return loss_log\n",
    "\n",
    "### 2.3 Training the model<a href=\"#2.3-Training-the-model\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "Instructions:\n",
    "\n",
    "To train a model, change the model\\_name to whatever you want to call\n",
    "your model and run. The model and loss will be saved after each epoch,\n",
    "so you can quit half way through the loop and the model will be saved.\n",
    "Then to continue training, uncomment lines 10-11 and the model will be\n",
    "loaded and training will continue.\n",
    "\n",
    "The loss values are stored as a csv in model\\_saves/loss\\_values.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    model = CNNSEG()\n",
    "    model_name = \"new_model\"\n",
    "    loss_log = [[]]\n",
    "\n",
    "    # Uncomment to load a previous model to train further\n",
    "    # model.load_state_dict(torch.load(\"model_saves/\"+model_name+\".pth.tar\"))\n",
    "    # loss_log = retrieve_loss(model_name)\n",
    "\n",
    "\n",
    "    weights = torch.tensor([0.023, 0.36,  0.313, 0.304])  # Accounts for difference in size of different classes\n",
    "    Loss = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = torch.optim.ASGD(model.parameters(), lr=0.1)\n",
    "    batch_size = 12\n",
    "\n",
    "    data_path = './data/train'\n",
    "    num_workers = 0  \n",
    "    epochs = 50\n",
    "\n",
    "    train_set = TrainDataset(data_path)\n",
    "    training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        if not (epoch == 0 and len(loss_log[0]) == 0):\n",
    "            loss_log.append([])\n",
    "\n",
    "        for iteration, sample in enumerate(training_data_loader):\n",
    "            img, mask = sample\n",
    "            img = img.reshape((-1, 1, 96, 96))\n",
    "\n",
    "            model.train()     \n",
    "            optimizer.zero_grad() \n",
    "            pred_mask = model(img)\n",
    "            \n",
    "            loss = Loss(pred_mask.float(), mask.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_log[-1].append(loss.item())\n",
    "\n",
    "        save_loss(loss_log, model_name)\n",
    "        torch.save(model.state_dict(), \"model_saves/\"+model_name+\".pth.tar\")\n",
    "\n",
    "3 Validation<a href=\"#3-Validation\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------\n",
    "\n",
    "### 3.1 Validation metrics<a href=\"#3.1-Validation-metrics\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "Validation is performed using the categorical\\_dice method provided as\n",
    "this is what the kaggle meaasurement uses.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # Useful functions\n",
    "\n",
    "    def categorical_dice(mask1, mask2, label_class=1):\n",
    "        \"\"\"\n",
    "        Dice score of a specified class between two volumes of label masks.\n",
    "        (classes are encoded but by label class number not one-hot )\n",
    "        Note: stacks of 2D slices are considered volumes.\n",
    "\n",
    "        Args:\n",
    "            mask1: N label masks, numpy array shaped (H, W, N)\n",
    "            mask2: N label masks, numpy array shaped (H, W, N)\n",
    "            label_class: the class over which to calculate dice scores\n",
    "\n",
    "        Returns:\n",
    "            volume_dice\n",
    "        \"\"\"\n",
    "        mask1_pos = (mask1 == label_class).astype(np.float32)\n",
    "        mask2_pos = (mask2 == label_class).astype(np.float32)\n",
    "        dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "        return dice\n",
    "\n",
    "\n",
    "    def get_scores(mask, pred_mask, print_scores=True):\n",
    "        \"\"\" Prints and returns the dice scores for a set of predicted masks\"\"\"\n",
    "        dice_scores = []\n",
    "        for i in range(1, 4):\n",
    "            dice_scores.append(categorical_dice(mask.numpy(), pred_mask.numpy(), label_class=i))\n",
    "        dice_scores = np.asarray(dice_scores)\n",
    "\n",
    "        if print_scores:\n",
    "            print(\"Mean Dice scores for each class: \", dice_scores, \"  Mean Dice Score across all classes: \", np.mean(dice_scores))\n",
    "\n",
    "        return dice_scores, np.mean(dice_scores)\n",
    "\n",
    "### 3.2 Validation loop<a href=\"#3.2-Validation-loop\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "This code tests the model \"model\\_name\" on the validation set. The\n",
    "validation images are fed to the model, then the output masks are scored\n",
    "using the dice score function above. Each of the 3 relevant classes\n",
    "(excluding the first class) are scored individually and the mean score\n",
    "across these classes is also shown. Theres also a loop to print a few\n",
    "examples of predicted masks with the orignal image and target mask.\n",
    "\n",
    "To run this, all you need to do is change the model\\_name.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import numpy as np\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # Get data\n",
    "    data_path = './data/val'\n",
    "    val_set = TrainDataset(data_path)\n",
    "\n",
    "    num_workers = 0 \n",
    "    batch_size = len(val_set)\n",
    "    training_data_loader = DataLoader(dataset=val_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    model_name = \"new_model\"\n",
    "    model = CNNSEG()\n",
    "    model.load_state_dict(torch.load(\"model_saves/\"+model_name+\".pth.tar\"))\n",
    "    model.eval()\n",
    "\n",
    "    for iteration, sample in enumerate(training_data_loader):\n",
    "        img, mask = sample\n",
    "        img = img.reshape((-1, 1, 96, 96)) \n",
    "        output = model(img)  \n",
    "\n",
    "        # Transforms one hot encoded -> ordinal encoded\n",
    "        pred_mask = torch.from_numpy(np.argmax(output.data.numpy(), axis=1))  \n",
    "        get_scores(mask, pred_mask)\n",
    "\n",
    "4 Testing<a href=\"#4-Testing\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------\n",
    "\n",
    "### 4.1 Producing the segmentation mask png files<a href=\"#4.1-Producing-the-segmentation-mask-png-files\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import numpy as np\n",
    "    from torch.autograd import Variable\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torchvision.utils import save_image\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
    "    # produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
    "    data_path = './data/test'\n",
    "    num_workers = 0\n",
    "\n",
    "    test_set = TestDataset(data_path)\n",
    "    batch_size = len(test_set)\n",
    "    test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model_name = \"optimised_model_50\"\n",
    "    model = CNNSEG()\n",
    "    model.load_state_dict(torch.load(\"model_saves/\"+model_name+\".pth.tar\"))\n",
    "    model.eval()\n",
    "\n",
    "    for iteration, sample in enumerate(test_data_loader):\n",
    "        img = sample\n",
    "        img = img.reshape((-1, 1, 96, 96)) \n",
    "        output = model(img)\n",
    "        pred_mask = torch.from_numpy(np.argmax(output.data.numpy(), axis=1)) \n",
    "\n",
    "    for img_num in range(len(test_set)):\n",
    "        mask = pred_mask[img_num,:,:].numpy()\n",
    "        cv2.imwrite(\"predicted_masks/cmr\"+str(121+img_num)+\"_mask.png\", mask)\n",
    "\n",
    "### 4.2 Exporting the predictions to csv files<a href=\"#4.2-Exporting-the-predictions-to-csv-files\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import cv2\n",
    "\n",
    "    def rle_encoding(x):\n",
    "        '''\n",
    "        *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
    "        x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "        Returns run length as list\n",
    "        '''\n",
    "        dots = np.where(x.T.flatten() == 1)[0]\n",
    "        run_lengths = []\n",
    "        prev = -2\n",
    "        for b in dots:\n",
    "            if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "            run_lengths[-1] += 1\n",
    "            prev = b\n",
    "        return run_lengths\n",
    "\n",
    "\n",
    "    def submission_converter(mask_directory, path_to_save):\n",
    "        writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
    "        writer.write('id,encoding\\n')\n",
    "\n",
    "        files = os.listdir(mask_directory)\n",
    "\n",
    "        for file in files:\n",
    "            name = file[:-4]\n",
    "            mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            mask1 = (mask == 1)\n",
    "            mask2 = (mask == 2)\n",
    "            mask3 = (mask == 3)\n",
    "\n",
    "            encoded_mask1 = rle_encoding(mask1)\n",
    "            encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
    "            encoded_mask2 = rle_encoding(mask2)\n",
    "            encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
    "            encoded_mask3 = rle_encoding(mask3)\n",
    "            encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
    "\n",
    "            writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
    "            writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
    "            writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # Creates csv file for upload to Kaggle\n",
    "    submission_converter(\"predicted_masks\",\"submission\")"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
